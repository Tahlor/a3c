# a3c
Implementation of A3C algorithm for cryptotrading.

Accurate predictions of market behavior are akin to the holy grail in the finance world. The problem of divining the future from past market behavior has been studied extensively with varying success. In this paper, we use the Asynchronous Advantage Actor-Critic, to cast the problem as a game and learn how to “win” using reinforcement learning. We implement both a naive version with multilayer perceptrons and a recurrent version with gated recurrent units for both the actor and critic networks. We compare our systems’ results with the results from a simple buy-and-hold strategy, and find that both outperform this baseline strategy, with the recurrent model performing better on average than the naive implementation.

# Background

The problem of predicting market behavior is notoriously difficult, as evidenced by the well-known disclaimer “past performance is no guarantee of future results.”

Since, intuitively, we do not know beforehand which action to take, it is difficult to define target values for an automated learner. This naturally suggests that an unsupervised approach may be appropriate for finding a solution. Further, unlike traditional assets, we note that cryptocurrencies do not have very robust valuation metrics and are inherently more speculative. Consequently, while their valuation is impacted by new information, it stands to reason that day-to-day changes in their valuation are primarily driven by traders and algorithms reacting to momentum, small changes in price, and sentiment, rather than new external information. This suggests an agent may be able to learn when to buy or sell the asset based only on previous price information while obtaining some alpha in the short run. We believe this trading strategy may be more difficult to implement successfully with more traditional asset classes, where valuations are generally less volatile, and large price swings may be more frequently affected by external information in the short run.
Reinforcement learning, with its try-and-see approach to learning, thus seems well suited to model the problem of cryptocurrency trading. We believe that there is little to no external information required beyond the price history.

For this work, we chose to implement the Asynchronous Advantage Actor-Critic framework, commonly referred to as A3C [3]. This framework consists of a policy that determines which actions should be chosen in a particular state, a value function that assigns a value to a state, and a loss function that focuses on the advantage gained by the actions the agent took. The policy can be viewed as the actor and the value function as the critic in an actor-critic architecture; hence we consider the terms “policy” and “value” in this paper to be interchangeable with “actor” and “critic”, respectively. Though the policy and the value function can be complex and calculation of exact values for each can be intractable, they can be approximated with a neural network, the trainable weights of the network being the parameters that are used to choose an action or evaluate a state.

Although the policy and value networks are conceptually separate, they can be combined in cases where the environment is complex and the process of producing features to determine an action or a value is computationally expensive. When the two networks are combined, the network output is typically fed through separate fully-connected layers of nodes to map the shared network output to either the action space or the value space. These calculated actions and values are then used to determine gradients for updating the original networks. In our models, detailed in section V, we used separate policy and value networks.

One performance enhancing aspect of A3C is its asynchronous nature. Global policy and value networks are constructed, and then copied into worker threads that explore the feature space by iteratively playing games in parallel. Since actions are chosen stochastically by sampling around a learned mean value, and beginning states may also be chosen randomly, each thread has an opportunity to explore a different region in the feature space. As threads complete a predefined number of games, gradients are calculated based on the actions they took and the expected value of the states they visited. These gradients are then used to update the global policy and value networks, which are subsequently copied down to the threads again, overwriting their local copies. This asynchronous process of exploration and updating has been shown to provide a significant speedup in learning difficult problems, such as maximizing player score in Atari games.

# Data

Complete transactional trade data was obtained from GDAX from December 2014 through March 2018, consisting of nearly 40 million transactions. Data fields included the trade price, volume, time of the transaction, and whether it was a “buy” or “sell” side order.

# Model Definition
We created two models using the A3C framework. The first was a naive, shallow implementation using a simple fully connected layer for the actor and critic networks. Because there is a strong temporal aspect of this problem, we included past price changes as feature inputs in the naive model. We separately implemented a recurrent neural net approach, as we expected this would be better adapted to dealing with the temporal component.
We intended to use the minimal possible state definition, in which a state would simply consist of a single price. However, we quickly discovered that this was not sufficient, and so we added transactional information (i.e., whether the current translation was a market buy or a market sell). Sequences of transactions, as represented by the trade price reported at the time of the transaction, are grouped together into “games” of arbitrary length. The object of each game is to maximize total net worth, which is defined as

wt =ct+pt ⋅ at,

where, at time step t, ct is the cash available to the model, pt is the current market price, and at is the current number of assets held by the model. For each model, the starting cash amount (c0) was set at $10,000.

At each time step, our model produces a single action measured on a continuous scale between -1 and 1, inclusive. An action of -1 corresponds to selling all assets that the model holds at that time step; similarly, an action of 1 corresponds to buying all the assets the model can afford at that time step. Fractional values correspond to buying or selling some fraction of the total available assets. An action of 0 corresponds to doing nothing.

As a proof of concept, we first implemented A3C with simple perceptrons as the policy and value networks. To provide some context to the prediction task in this case, we constructed vectors of ten sequential prices to be presented to the policy and value networks. While this model performed reasonably well, we felt that we could do better with recurrent networks, given that there is a significant temporal aspect to our cryptocurrency problem that we felt was not captured well by simply gathering a batch of data points into a single vector.

Originally, we chose to use the classic long short-term memory (LSTM) architecture in our policy and value networks. However, we found that the separation of context and hidden state in the LSTM was undesirable for our use case, and we abandoned the LSTM fairly quickly. Since the gated recurrent unit (GRU) has been shown to perform as well as the LSTM [2], while training faster and combining both context and hidden state into a single vector, we settled on the GRU as the basis for our recurrent implementation.

All of these implementations were created using Google’s TensorFlow machine learning framework [1]. It is worth mentioning that recurrent networks introduce a certain amount of complexity into training. Rather than simply feeding in an input at every time step, recurrent structures in TensorFlow require that a vector representing the state of the unit be passed as well. This significantly impacted our training time, as we could no longer produce results for multiple inputs at once. Rather, we were forced to feed in a single input at a time with its associated context vector so that we could capture the resulting context vector after the input was processed and feed that in with the next input. This caused the training process for the recurrent model to be significantly slower than the naive model.
